==================
SCHROEDINGER'S BOX
==================


Basic Idea
==========

Imagine a physical object that doesn't show its inner structure, for example by an opaque surface, and also the shape of the object itself doesn't give a hint on its material, meaning, and purpose... Something like the black monolith in Stanley Kubrick's "2001: A Space Odyssey". How would we expect it to sound like, if we hit it with a mallet? What kind of sound would be plausible, and what would be implausible?
In other words, we want to explore the perceptual plausibility of auditory feedback caused by hitting/striking/tapping interaction, in case there is not much information given from other sensory modalities (vision, touch).

The project involves the development of a simple hardware platform based on a single-board computer (Bela platform [1]) with attached piezo-electric contact microphones and miniature loudspeakers; programmed in the SuperCollider language; battery-powered, all hidden inside a cube painted in black. For the electronics part, everything is already there, no new development is needed. If interaction is limited to impacts (no scratching, rubbing, etc.), sounds can be just pre-produced samples (with a bit of randomization so that different strikes sound different). Signal processing is done in SuperCollider. It should detect impacts (onset detection) as fast as possible, roughly predict their velocity (mapped at least to amplitude), and play back pre-recorded samples. Samples are used instead of physical modelling sound synthesis, to simply allow using all kinds of sounds, even very implausible, with the available limited computing power of a small single-board computer.

The box is painted in shiny black, like a piano, so that is not even possible to get sound from scratching. Under this aspect, even hand interactino might be possible (tapping, knocking) if the onset and amplitude detection works well enough, even for very soft sounds.


Sounds and parameter space
==========================

There are various versions of timbre spaces. For example, the model by [Gounaropoulos2006] includes timbre dimensions (bright, warm, harsh) as well as physical properties (metallic, plucked, etc.). I would also define a similar kind of parameter space, but stick to only physical parameters. These should include basic physical properties that concern material, hollow/solid, geometry (size/shape), etc. For such interaction sounds I can draw on a physical model to create also in-between objects. However, my hypothesis is that most of the combinations would be technically feasible to build, and are therefore more or less similar in plausibility. Maybe only for extreme materials such as diamond, it might get implausible.

As I would like to explore the limits of plausibility, we need to go further. There are a lot of other sounds that might be actually technically feasible but are not covered by a standard timbre space. For example, if the object is bigger than a cat, it might be plausible, if it makes "miao" if the object is hit (in addition to the sound of a hollow cardboard box). My hypothesis is that this won't be completely implausible, but the high complexity of explanation (cat locked inside the box) reduces the plausibility of the sound. If, however, the object roars like a tiger, this should be completely implausible, as it is physically impossible.

For synthetic sounds that don't mimic any physical system, I would assume two possible ways of perception.
One is that the sound is identified as artificial, and the only plausible explanation is that of hidden electronics inside the object. Under the assumption that everything can be faked, a person will accept just anything as similarly plausible, and the question of plausibility actually gets meaningless.
The other way of perception would be that the listener is not really aware that the sound is synthetic, but perceives it as plausible, because it is a cartoonification of a very plausible sound. Some examples are given in [Wages2005]. However, this is usuially strongly dependent to the context. While watching a movie or playing a computer game, a cartoonification might work even better that an actual recording, but if regarded in isolation, it might suddenly become implausible. As in the currently considered scenario we have no context at all, we should perhaps just forget this case.

For synthetic sounds that mimic a physical system, most literature comes to the conclusion that we cannot discriminate between real and synthetic, even with very simplified modal synthesis models. These models are widely used in psychoacoustic experiments. I would therefore also include sounds generated by a physical model, in order to have full control over physical parameters.

Literature
----------
[Gounaropoulos2006] Synthesising Timbres and Timbre-Changes from Adjectives/Adverbs
[Wages2005] How Realistic is Realism? Considerations on the Aesthetics of Computer Games


Plausibility
============

What we would like to get from participants is a rating of plausibility on a continuous scale, e.g., between 0 (totally implausible) and 1 (totally plausible). However, nobody really knows what plausibility actually means, and what would be what value, so we actually don't know what we are measuring.

There is not much literature on plausibility.

[Lindau2012] solve the issue by transforming it into a signal detection task. For each sound, participants are asked if this is a real recording or a synthesized soundscape. Other than us, they don't have a physical object as reference. We could apply their paradigm in modified form with the question: "Would you believe it, if you found this object on the streets, and it sounded like that?"

[Connel2006] introduced a model for plausibility. According to them, a plausible scenario (in our case: combination between sound and sight/touch) is one that fits prior knowledge
(1) using many different sources of corroboration. That is, the scenario should have several distinct pieces of prior knowledge supporting any necessary inferences.
(2) without complex explanation. That is, the scenario must be represented without relying on extended or convoluted justifications.
(3) using minimal conjecture. That is, the scenario must be represented by avoiding, where possible, the introduction of hypothetical entities (i.e., no deus ex machina).

plausibility = 1-implausibility
implausibility = complexity/(corrobation-conjecture)

This leads to the second paradigm for the experiment. We might ask participants to provide a physical explanation for the sound. From the answers, we might be able to compute a plausibility value.

The problem is, how to quantify the three parts for example the "complexity of explanation". By number of words needed to explain it?

"Corrobation" might be measured as the causal uncertainty, i.e., how many different possible answers are given by the participants for the same sound. If participants come up with many different answers, then there are many possible plausible explanations, meaning high corrobation?

"Conjecture": I would interpret this as "the box sounds like a cat because in the same moment I hit it, someone behind me hits a cat." Maybe this doesn't apply to our case. Maybe just use a simplified formula without conjecture.


Literature
----------
[Lindau2012] Assessing the plausibility of virtual environments
[Connell2006] A model for plausibility



Experiment
==========

In the experiment, I would therefore ask participants not only to rate plausibility (whatever that actually means for someone), but rather let them describe the material and contents of the object in a few words. The entropy of these descriptions across participants might give a hint on the plausibility, analog to the causal uncertainty measure that is derived from free verbalizations of sounds [Ballas1986].
See also [Lemaitre2010] for more recent research on causal uncertainty.


Literature
----------
[Ballas1986] Causal Uncertainty in the Identification of Environmental Sounds
[Lemaitre2010] Listener expertise and sound identification influence the categorization of environmental sounds.




Onset detection
===============

Requirements
------------

* round-trip latency must be altogether (including AD/DA conversion, buffers, etc.) below 10ms
* latency must be constant (not sometimes 5ms, sometimes 8ms): below 1ms jitter
* Parameters we want to obtain: amplitude of the onset


[Jack2016] found that random latency between 7ms and 13ms feels as bad as 20ms constant latency; but random latency between 9ms and 11ms is not discriminable from 10ms constant latency.
Most literature sources agree that overall latency must be less than 10ms.
However, all these studies use a setup where participants hear only the synthesized sound. In our case, the sound needs to merge with the original impact sound in a way that only one single sound is perceived. Under these circumstances, 10ms are maybe not fast enough.



Literature
----------
[Jack2016] Effect of latency on performer interaction and subjective quality assessment of a digital musical instrument




Simplest onset detector
-----------------------

Signal --> squared or abs --> envelope follower --> threshold detection

* envelope follower: Amplitude.ar in SC. Instant attack but smoothed decay.
* threshold detection: Schmidt.ar in SC, with lower and higher threshold set to the same value, will output 1 if signal is above the threshold, and 0 if the signal is below the threshold.
* the output signal can then be used to trigger a sampler that reacts on the 0-to-1 transition.



Threshold detection with hysteresis
-----------------------------------

An input signal with strong amplitude modulation might falsely retrigger at every peak.
Solution: Onsets are only detected after the signal has dropped below a second threshold, lower than the onset detection threshold.
See Schmitt-Trigger: https://en.wikipedia.org/wiki/Schmitt_trigger
In SC: Schmidt.ar



Trend subtraction
-----------------

Signal --> squared --> envelope follower --> trend subtraction --> half-wave rectification --> threshold detection

* similar to "Coyote" object in SC (see help-file for description)
* envelope gets smoothed by a fast and a slow lowpass-filter (Lag object in SC)
* trend subtracton: gain*fast - slow
* the gain should be below 0 and 0.9, so that its output is always below the slowly-smoothed signal, except on onsets.


High-pass filtering
-------------------

Filter the input signal by a high-pass filter (use 4kHz cutoff frequency as a starting value), before onset detection.


Hard real-time onset detection
------------------------------

Check [Turchet2018] if he has some other tricks.


Bandpass-filterbank
-------------------

* See [Klapuri1999]. Instead of FFT, we can split the signal into frequency bands by using a bandpass filterbank.
* For example, a Gammatone filterbank (Gammatone.ar in SC).
* The onset detection is performed in each band individually in parallel, and a real onset is only detected, if it occurs in many bands at the same time.
* This way, a sharp but not very loud transient can be detected, even in the presence of a loud resonating sound loud tail of impact before that still resonates.
* Note that Bela is very limited in computing power, so it won't be possible to process many bandpass channels. Only a handful. Needs to be tried out.


Combine with FFT-based onset detection
--------------------------------------

Idea: If an onset is detected with the fast time-domain method, already start playback of the sample, even if we are not sure if it was a real onset.
Then, a parralel (slow) FFT-based onset detector checks the same signal. If it doesn't detect an onset, the sample playback is cancelled. If it agrees with the detection, do nothing.


Amplitude-estimation
--------------------

The peak of the transient is not yet reached when the onset is detected. The amplitude is thus still unknown.
Anyway, start playback of the sample already, with default amplitude. Wait a few milliseconds after onset detection and select the biggest value since the onset as amplitude.

Use the RunningMax object in SC.
As input take the fast envelope follower from the trend-subtraction part.
As trigger signal for reset, use the output of the onset detection. The runningmax will thus be reset on every onset.
Get the value after x ms as amplitude value.


Literature
----------
[Dixon2006] Onset Detection Revisited
[Turchet2018] Hard real-time onset detection for percussive sounds
[Klapuri1999] Sound onset detection by applying psychoacoustic knowledge



Sample Player
=============

On each onset, a sample is played with the given amplitude.

Requirements:
* Amplitude (maybe even amplitude adjustment after triggering)
* The possibility to cancel playback, i.e., fade out quickly (if it was detected by fast algorithm, but afterwards the slow algorithm rejected it)
* Randomization: there are a few versions for each sample. Take one randomly for each impact, with the additional rule that the next sample differs from the previous sample.
* If also impact position is tracked (maybe by optical tracking with a camera?), then the sound could be filtered according to the position, so that the sound differs between different positions, but always the same at a single position.
* Polyphony: For impact sounds with long decay, a new impact should not cancel the old but add to it. For complex sound constructs such as impact+cat, maybe the cat should miao only once at a time to stay plausible...

Polyphony and randomization are hard do accomplish in the audio signal domain. We therefore need to get back to message domain. This might take some samples time, instead of staying in the dsp loop, but we have no choice.
In SC: SendTrig.ar sends a trigger message (on every non-positive to positive signal change) from the server back to the language.
This trigger message can then be used to trigger the polyphonic sample player, i.e., spawn a new synth on the server for every impact.


Ideas for sound variability
---------------------------

In order to achieve that every single hit sounds different, even if it is always the same sample, some simple "effects" could be applied. The parameters should in general be randomly chosen at the start of the sample and then stay constant (the object shouldn't change over time).

* randomize playback speed (changes pitch and duration) (physical explanation: different size or material)
* random comb filter
** physical explanation: excitation at different positions leads to attenuation at certain frequencies
** H(z) = 1-z^{pN+0.5}, where N is the pitch period in samples, and p is the position between 0 and 1.
** N is period of base frequency in samples. Just assume 100Hz --> 0.01 seconds.
** Implentation in SC: "signal - DelayN(signal, maxdelaytime: 0.01, delaytime: (p*0.01)+(0.5/48000))". With "DelayN", delay can only happen in steps of 1 sample. But that is enough as we don't need a smooth change over time. Otherwise costly interpolation would be necessary. Randomize playback position between 0 and 1 with "p=1.0.linrand".


Voice stealing
--------------

* In order to limit computing power, the maximum number of voices playing at the same time are limited.
* If the maximum is reached, a new trigger will steal the voice of the oldest running voice.
* Best practice would be to not just cancel it, but to trigger its release, letting it fade out so that no click is produced when just cutting it off. This means, however, that the old voice still runs (for the fadeout) while the new is already triggered. The stealing must therefore happen if there is still one voice free. Maybe 4 voices maximum is a good start.
* In SC: use Voicer from the ddwVoicer Quark.

The maximum number of voices should actually be a parameter that is set individually for each sound. For some sounds, such as the cat, there is no polyphony. A cat can only miao once at a time. For basic physical objects, however, individual impacts will add up.


Velocity
--------

* The velocity (= amplitude of the sample) is controlled by the measured velocity of the onset.
* Find the right mapping between the measured velocity and the playback amplitude. Easiest would be linear.
In SC: map between parameter ranges with linlin (linear input range to linear output range), or linexp (linear input, exponential output), or other types.




Sample randomization
--------------------

* on each hit, a random version of the chosen sound should be selected.
** additional condition: the new sample should never be the same as the previous one

Possible implementation in SC:

~newID = 0; // reset sample ID counter
~n = 5; // number of samples
// create n urns where always one of the 5 is missing (all combinations)
~urn = n.collect{|i|
	var foo = (0..n-1);
	foo.takeAt(i);
	foo;
};
~newID = ~urn[~newID][(n-1).rand] // take a random sample from the one urn that lacks the sample that has been played the last time.




Samples for the experiment
==========================

Task: Create sample library that covers the whole parameter space equally, with a limited set of about 50? different sounds.

Imagine a parameter space that is defined by a finite number of sound parameters (e.g., pitch, duration, ..., saltyness, ..., miaouiness, ...) and that contains all sounds in the world. Every sound is defined by its position in the parameter space, i.e., its parameter values. These parameters are actually unknown to us. We now want a subset of about 50? sounds that represent the whole parameter space. In other words: we select sounds that are equally spread within the space.

For each sound, we want a couple (e.g., 4) different versions so that not every hit sounds the same. The best way to achieve this is to record it several times.

Soundfiles should be cut tightly at the start. Exactly at a zero-crossing, where the signal changes from negative to positive or vice-versa. The first sample of the signal should be the one after the zero-crossing.

Soundfiles should be cut in the end after the signal has vanished into the general noise-floor. Apply a fade-out in the end, after the signal has vanished, to fade out the noise floor.


technical
---------
* do it like that: https://www.youtube.com/watch?v=dzSZQWzMGZg

* Here it is described how to used the Zoom H6: https://www.youtube.com/watch?v=wa5sI8EcT7s

* record stereo, at 48kHz, 24bit
* use XY mic set to 90° angle
* optimal: place the recorder on a mic stand, so that you have free hands.
* position everything so that the object is in front of the microphones. not too near, not too far. try about 30cm, depending on size of the object.
* level the input gains to the loudness of sound you want to record. try it out and set gains so that the sound is loud, but you have still a few dB headroom. What you want is maximum gain, just before it clips. Level should be somewhere between -12 and -6dB.
* if recording outside, you need to put on the windscreen.


Hardware
========

Electronics
-----------
* BeagleBone Black micro computer + CTAG FACE cape (8 audio out, 4 audio in at 48kHz) + Bela cape (4 non-audio Analog i/o at 48kHz)
* 4 piezo contact microphones (raw piezo disks)
* 2x 2-channel piezo buffer (impedance matching): Schatten Design MicroPre2, powered by 9V
* voltage regulator to power piezo buffer from 5V: https://www.neuhold-elektronik.at/catshop/product_info.php?products_id=5478
* 4 structure-borne exciters (Visaton EX-30S or EX-45S or EX-60S)
* 4ch audio amplifier (2x https://www.neuhold-elektronik.at/catshop/product_info.php?products_id=7429)
* Power supply via 10Ah powerbank (Shanqiu FX 5-12 Mini UPS Backup Battery) with separate outputs for 5V (Bela), 9V (piezo buffer), and 12V (amp).

Additional electronics for accelerometer
----------------------------------------
* 3-axes accelerometer (only translation, no rotation) with analog output per axis (GY-61 ADXL335 sensor)
* connected to Bela through analog inputs as in the example (https://learn.bela.io/tutorials/pure-data/sensors/accelerometer/). [x,y,z] connect to channels [0,1,2].


Wireless network
----------------
* Wireless router (TP-Link TL-WR902AC) creates a local wireless network (ssid: schroedinger, pw: erwindschroedinger)
* Bela automatically connects to the WLAN via USB-to-WLAN dongle (Xoro HWL 155N or WeChip 150M STB WiFi Adapter). From several tested dongles, only those with Ralink RT5370 chipset worked out of the box.
* Bela can be controlled by any computer within the network. Bela is identified as "bela.local" or by IP (see below).
* The router assigns fixed IPs to the known devices (Bela: 192.168.0.100, Jason's laptop: 192.168.0.101, Marian's laptop: 192.168.0.102)



Box
---

Requirements:
* ca. 25x25x25 cm cube
* top plate of solid material that is decoupled from the rest of the cube (e.g., by a rubber layer), should have a solid feel and not radiate so much sound.
* side plates of thin material (e.g., thin wood or aluminium) to allow excitation through exciters
* Bottom maybe open to allow a (hidden) subwoofer (would need additional amplifier)?
* painted in shiny black from outside, so that no visual information shows anything about material, contents, etc.

here are some ideas for constructing the cube as simple as possible:

* based on acryl cube (ok, 100EUR is pricy): https://www.boesner.at/acrylglashaube-fuer-galerie-sockel-und-skulpturenstaender-25079
** the open side of it (originally bottom) is used as the top where a top plate is placed on rubber.
** a hole is made into the now closed bottom plate to allow service and possibly subwoofer
** the top plate can be something like mdf wood.

* based on pre-constructed painting canvas (5mm mdf plate on wooden frame) https://www.boesner.at/gesso-malplatte-26240
** the 5mm mdf might be thin enough to allow radiation with the exciters
** the frame allows to combine 4 of these nicely as a basis for the cube
** the frames are already nicely finished to allow painting
** on the top, a simple mdf plate (thicker, maybe 1cm)  is placed on rubber feet, the gaps filled with silicone
